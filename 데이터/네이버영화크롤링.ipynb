{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#실패\" data-toc-modified-id=\"실패-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>실패</a></span></li><li><span><a href=\"#영화별-크롤링\" data-toc-modified-id=\"영화별-크롤링-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>영화별 크롤링</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실패\n",
    "- 1000P까지만 가능\n",
    "- 네이버에서 막아놓음\n",
    "- 다른 방법으로 크롤링하자!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "def max_page(url):\n",
    "    res = requests.get(url)\n",
    "    soup = BeautifulSoup(res.content, 'html.parser')\n",
    "    return int(soup.find('strong', class_='c_88 fs_11').text) // 10 + 1\n",
    "\n",
    "url = 'https://movie.naver.com/movie/point/af/list.nhn'\n",
    "def crawling_data(url):\n",
    "    df = pd.DataFrame(columns = ['title_num', 'title', 'review', 'auther'])\n",
    "    max_p = max_page(url)\n",
    "    error_li = []\n",
    "    for i in range(1, max_p):\n",
    "        if i % 10 == 0:\n",
    "            print(i)\n",
    "        params = {'page' : i}\n",
    "        res = requests.get(url, params = params)\n",
    "        soup = BeautifulSoup(res.content, 'html.parser')\n",
    "\n",
    "        tr_li = soup.find('tbody').find_all('tr')\n",
    "        for idx, tr in enumerate(tr_li):\n",
    "            try:\n",
    "                now = dict()\n",
    "                now['title_num'] = tr.find('td', class_='ac num').get_text()\n",
    "                now['title'] = tr.find('a', class_='movie color_b').get_text()\n",
    "                now['review'] = ''.join(tr.find('a', class_='report').get('href').split(\",\")[2:-2]).strip()[1:-1]\n",
    "                now['auther'] = tr.find('a', class_='author').get_text()\n",
    "                df = df.append(now, ignore_index = True)\n",
    "            except:\n",
    "                print('error', f\"{i},{idx}\")\n",
    "                error_li.append([i, idx])\n",
    "\n",
    "    return df, error_li\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 영화별 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "def max_page(url):\n",
    "    res = requests.get(url)\n",
    "    soup = BeautifulSoup(res.content, 'html.parser')\n",
    "    q, r = divmod(int(soup.find('strong', class_='c_88 fs_11').text), 10)\n",
    "    return q+1 if r else q\n",
    "\n",
    "def crawling_data_by_movie(number):\n",
    "    try:\n",
    "        number = int(number)\n",
    "        base_url = 'https://movie.naver.com/movie/point/af/list.nhn?st=mcode&' + f\"sword={number}\" + '&target=after'\n",
    "        max_p = max_page(base_url) + 1\n",
    "\n",
    "        df = pd.DataFrame(columns = ['number', 'title', 'score', 'review', 'auther'])\n",
    "        error_li = []\n",
    "    except:\n",
    "        return pd.DataFrame(columns = ['number', 'title', 'score', 'review', 'auther']), []\n",
    "    for i in range(1, max_p):\n",
    "#         if i % 100 == 0:\n",
    "#             print(i)\n",
    "        params = {'page' : i}\n",
    "        res = requests.get(base_url, params = params)\n",
    "        soup = BeautifulSoup(res.content, 'html.parser')\n",
    "\n",
    "        tr_li = soup.find('tbody').find_all('tr')\n",
    "        for idx, tr in enumerate(tr_li):\n",
    "            try:\n",
    "                now = dict()\n",
    "                now['number'] = tr.find('td', class_='ac num').get_text()\n",
    "                now['title'] = tr.find('a', class_='movie color_b').get_text()\n",
    "                now['score'] = tr.find('em').get_text()\n",
    "                now['review'] = ''.join(tr.find('a', class_='report').get('href').split(\",\")[2:-2]).strip()[1:-1]\n",
    "                now['auther'] = tr.find('a', class_='author').get_text()\n",
    "                df = df.append(now, ignore_index = True)\n",
    "            except:\n",
    "                error_li.append([i, idx])\n",
    "\n",
    "    return df, error_li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = crawling_data_by_movie(11206)\n",
    "print(b)\n",
    "print(a.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[145, 9]]\n",
      "[[30, 4]]\n",
      "[[60, 3]]\n",
      "[[427, 0]]\n",
      "11000\n",
      "[[4, 1]]\n",
      "12000\n",
      "[[79, 1]]\n",
      "[[23, 5]]\n",
      "13000\n",
      "[[130, 4]]\n",
      "[[62, 8]]\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "[[38, 7]]\n",
      "17000\n",
      "[[16, 9]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[65, 9]]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-176554a1f0e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mnow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrawling_data_by_movie\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-04a26801cb64>\u001b[0m in \u001b[0;36mcrawling_data_by_movie\u001b[0;34m(number)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mtr_li\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tbody'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_li\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "# 10001이 처음이고 199900이 마지막임 11월 6일 기준\n",
    "df = pd.DataFrame(columns = ['number', 'title', 'score', 'review', 'auther'])\n",
    "for num in range(10_001, 199_901):\n",
    "    if num % 1000 == 0:\n",
    "        print(num)\n",
    "    now, error = crawling_data_by_movie(num)\n",
    "    if len(now) > 100:\n",
    "        df = df.append(now, ignore_index = True)\n",
    "        if len(error):\n",
    "            print(error)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "274775"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "df.to_pickle('data.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "519"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['title'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "315.992px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
