{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "class DeepCoNN(object):\n",
    "    def __init__(\n",
    "        self, user_length,item_length, num_classes, user_vocab_size,item_vocab_size,\n",
    "        fm_k,n_latent,user_num,item_num,embedding_size, filter_sizes, num_filters,\n",
    "        l2_reg_lambda=0.0,l2_reg_V=0.0\n",
    "    ):\n",
    "        self.input_u = tf.placeholder(tf.int32, [None, user_length], name=\"input_u\")\n",
    "        self.input_i = tf.placeholder(tf.int32, [None, item_length], name=\"input_i\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None,1],name=\"input_y\")\n",
    "        self.input_uid = tf.placeholder(tf.int32, [None, 1], name=\"input_uid\")\n",
    "        self.input_iid = tf.placeholder(tf.int32, [None, 1], name=\"input_iid\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "        with tf.name_scope(\"user_embedding\"):\n",
    "            self.W1 = tf.Variable(\n",
    "                tf.random_uniform([user_vocab_size, embedding_size], -1.0, 1.0),\n",
    "                name=\"W\")\n",
    "            self.embedded_user = tf.nn.embedding_lookup(self.W1, self.input_u)\n",
    "            self.embedded_users = tf.expand_dims(self.embedded_user, -1)\n",
    "\n",
    "        with tf.name_scope(\"item_embedding\"):\n",
    "            self.W2 = tf.Variable(\n",
    "                tf.random_uniform([item_vocab_size, embedding_size], -1.0, 1.0),\n",
    "                name=\"W\")\n",
    "            self.embedded_item = tf.nn.embedding_lookup(self.W2, self.input_i)\n",
    "            self.embedded_items = tf.expand_dims(self.embedded_item, -1)\n",
    "\n",
    "        pooled_outputs_u = []\n",
    "\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"user_conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_users,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, user_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs_u.append(pooled)\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.h_pool_u = tf.concat(3,pooled_outputs_u)\n",
    "        self.h_pool_flat_u = tf.reshape(self.h_pool_u, [-1, num_filters_total])\n",
    "\n",
    "        pooled_outputs_i = []\n",
    "\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"item_conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_items,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, item_length- filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs_i.append(pooled)\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.h_pool_i = tf.concat(3,pooled_outputs_i)\n",
    "        self.h_pool_flat_i = tf.reshape(self.h_pool_i, [-1, num_filters_total])\n",
    "\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop_u = tf.nn.dropout(self.h_pool_flat_u, 1.0)\n",
    "            self.h_drop_i= tf.nn.dropout(self.h_pool_flat_i, 1.0)\n",
    "        with tf.name_scope(\"get_fea\"):\n",
    "            Wu = tf.get_variable(\n",
    "                \"Wu\",\n",
    "                shape=[num_filters_total, n_latent],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            bu = tf.Variable(tf.constant(0.1, shape=[n_latent]), name=\"bu\")\n",
    "            self.u_fea=tf.matmul(self.h_drop_u, Wu) + bu\n",
    "            #self.u_fea = tf.nn.dropout(self.u_fea,self.dropout_keep_prob)\n",
    "            Wi = tf.get_variable(\n",
    "                \"Wi\",\n",
    "                shape=[num_filters_total, n_latent],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            bi = tf.Variable(tf.constant(0.1, shape=[n_latent]), name=\"bi\")\n",
    "            self.i_fea = tf.matmul(self.h_drop_i, Wi) + bi\n",
    "            #self.i_fea=tf.nn.dropout(self.i_fea,self.dropout_keep_prob)\n",
    "\n",
    "        \n",
    "        with tf.name_scope('fm'):\n",
    "            self.z=tf.nn.relu(tf.concat(1,[self.u_fea,self.i_fea]))\n",
    "\n",
    "            #self.z=tf.nn.dropout(self.z,self.dropout_keep_prob)\n",
    "\n",
    "            WF1=tf.Variable(\n",
    "                    tf.random_uniform([n_latent*2, 1], -0.1, 0.1), name='fm1')\n",
    "            Wf2=tf.Variable(\n",
    "                tf.random_uniform([n_latent*2, fm_k], -0.1, 0.1), name='fm2')\n",
    "            one=tf.matmul(self.z,WF1)\n",
    "\n",
    "            inte1=tf.matmul(self.z,Wf2)\n",
    "            inte2=tf.matmul(tf.square(self.z),tf.square(Wf2))\n",
    "\n",
    "            inter=(tf.square(inte1)-inte2)*0.5\n",
    "\n",
    "            inter=tf.nn.dropout(inter,self.dropout_keep_prob)\n",
    "\n",
    "            inter=tf.reduce_sum(inter,1,keep_dims=True)\n",
    "            print (inter)\n",
    "            b=tf.Variable(tf.constant(0.1), name='bias')\n",
    "            \n",
    "\n",
    "            self.predictions =one+inter+b\n",
    "\n",
    "            print (self.predictions)\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            #losses = tf.reduce_mean(tf.square(tf.subtract(self.predictions, self.input_y)))\n",
    "            losses = tf.nn.l2_loss(tf.subtract(self.predictions, self.input_y))\n",
    "\n",
    "            self.loss = losses + l2_reg_lambda * l2_loss\n",
    "\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            self.mae = tf.reduce_mean(tf.abs(tf.subtract(self.predictions, self.input_y)))\n",
    "            self.accuracy =tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(self.predictions, self.input_y))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "UnrecognizedFlagError",
     "evalue": "Unknown command line flag 'f'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnrecognizedFlagError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a46fb4aac5fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mFLAGS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_compat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_flags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nParameters:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__flags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.8/site-packages/tensorflow/python/platform/flags.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m# a flag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_parsed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m       \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.8/site-packages/absl/flags/_flagvalues.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, argv, known_only)\u001b[0m\n\u001b[1;32m    643\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munknown_flags\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m       \u001b[0msuggestions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_helpers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_flag_suggestions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m       raise _exceptions.UnrecognizedFlagError(\n\u001b[0m\u001b[1;32m    646\u001b[0m           name, value, suggestions=suggestions)\n\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnrecognizedFlagError\u001b[0m: Unknown command line flag 'f'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf_compat\n",
    "import math\n",
    "import datetime\n",
    "import pickle\n",
    "flags = tf_compat.app.flags\n",
    "flags.DEFINE_string(\"word2vec\", \"../data/google.bin\", \"Word2vec file with pre-trained embeddings (default: None)\")\n",
    "flags.DEFINE_string(\"valid_data\",\"../data/music/music.valid\", \" Data for validation\")\n",
    "flags.DEFINE_string(\"para_data\", \"../data/music/music.para\", \"Data parameters\")\n",
    "flags.DEFINE_string(\"train_data\", \"../data/music/music.train\", \"Data for training\")\n",
    "\n",
    "# ==================================================\n",
    "\n",
    "# Model Hyperparameters\n",
    "#tf.flags.DEFINE_string(\"word2vec\", \"./data/rt-polaritydata/google.bin\", \"Word2vec file with pre-trained embeddings (default: None)\")\n",
    "flags.DEFINE_integer(\"embedding_dim\", 300, \"Dimensionality of character embedding \")\n",
    "flags.DEFINE_string(\"filter_sizes\", \"3\", \"Comma-separated filter sizes \")\n",
    "flags.DEFINE_integer(\"num_filters\", 100, \"Number of filters per filter size\")\n",
    "flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability \")\n",
    "flags.DEFINE_float(\"l2_reg_lambda\", 0.0, \"L2 regularizaion lambda\")\n",
    "flags.DEFINE_float(\"l2_reg_V\", 0, \"L2 regularizaion V\")\n",
    "# Training parameters\n",
    "flags.DEFINE_integer(\"batch_size\",100, \"Batch Size \")\n",
    "flags.DEFINE_integer(\"num_epochs\", 40, \"Number of training epochs \")\n",
    "flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps \")\n",
    "flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps \")\n",
    "# Misc Parameters\n",
    "flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "\n",
    "def train_step(u_batch, i_batch, uid, iid, y_batch, batch_num):\n",
    "    \"\"\"\n",
    "    A single training step\n",
    "    \"\"\"\n",
    "    feed_dict = {\n",
    "        deep.input_u: u_batch,\n",
    "        deep.input_i: i_batch,\n",
    "        deep.input_y: y_batch,\n",
    "        deep.input_uid: uid,\n",
    "        deep.input_iid: iid,\n",
    "        deep.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "    }\n",
    "    _, step, loss, accuracy, mae = sess.run(\n",
    "        [train_op, global_step, deep.loss, deep.accuracy, deep.mae],\n",
    "        feed_dict)\n",
    "    time_str = datetime.datetime.now().isoformat()\n",
    "\n",
    "    # print(\"{}: step {}, loss {:g}, rmse {:g},mae {:g}\".format(time_str, batch_num, loss, accuracy, mae))\n",
    "    return accuracy, mae\n",
    "\n",
    "\n",
    "def dev_step(u_batch, i_batch, uid, iid, y_batch, writer=None):\n",
    "    \"\"\"\n",
    "    Evaluates model on a dev set\n",
    "    \"\"\"\n",
    "    feed_dict = {\n",
    "        deep.input_u: u_batch,\n",
    "        deep.input_i: i_batch,\n",
    "        deep.input_y: y_batch,\n",
    "        deep.input_uid: uid,\n",
    "        deep.input_iid: iid,\n",
    "        deep.dropout_keep_prob: 1.0\n",
    "    }\n",
    "    step, loss, accuracy, mae = sess.run(\n",
    "        [global_step, deep.loss, deep.accuracy, deep.mae],\n",
    "        feed_dict)\n",
    "    time_str = datetime.datetime.now().isoformat()\n",
    "    # print(\"{}: step{}, loss {:g}, rmse {:g},mae {:g}\".format(time_str, step, loss, accuracy, mae))\n",
    "\n",
    "    return [loss, accuracy, mae]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    FLAGS = tf_compat.flags.FLAGS\n",
    "    FLAGS._parse_flags()\n",
    "    print(\"\\nParameters:\")\n",
    "    for attr, value in sorted(FLAGS.__flags.items()):\n",
    "        print(\"{}={}\".format(attr.upper(), value))\n",
    "    print(\"\")\n",
    "\n",
    "    print(\"Loading data...\")\n",
    "\n",
    "    pkl_file = open(FLAGS.para_data, 'rb')\n",
    "\n",
    "    para = pickle.load(pkl_file)\n",
    "    user_num = para['user_num']\n",
    "    item_num = para['item_num']\n",
    "    user_length = para['user_length']\n",
    "    item_length = para['item_length']\n",
    "    vocabulary_user = para['user_vocab']\n",
    "    vocabulary_item = para['item_vocab']\n",
    "    train_length = para['train_length']\n",
    "    test_length = para['test_length']\n",
    "    u_text = para['u_text']\n",
    "    i_text = para['i_text']\n",
    "\n",
    "    np.random.seed(2017)\n",
    "    random_seed = 2017\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        session_conf = tf.ConfigProto(\n",
    "            allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "            log_device_placement=FLAGS.log_device_placement)\n",
    "        session_conf.gpu_options.allow_growth = True\n",
    "        sess = tf.Session(config=session_conf)\n",
    "        with sess.as_default():\n",
    "            deep = DeepCoNN.DeepCoNN(\n",
    "                user_num=user_num,\n",
    "                item_num=item_num,\n",
    "                user_length=user_length,\n",
    "                item_length=item_length,\n",
    "                num_classes=1,\n",
    "                user_vocab_size=len(vocabulary_user),\n",
    "                item_vocab_size=len(vocabulary_item),\n",
    "                embedding_size=FLAGS.embedding_dim,\n",
    "                fm_k=8,\n",
    "                filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "                num_filters=FLAGS.num_filters,\n",
    "                l2_reg_lambda=FLAGS.l2_reg_lambda,\n",
    "                l2_reg_V=FLAGS.l2_reg_V,\n",
    "                n_latent=32)\n",
    "            tf.set_random_seed(random_seed)\n",
    "            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "\n",
    "            # optimizer = tf.train.AdagradOptimizer(learning_rate=0.1, initial_accumulator_value=1e-8).minimize(deep.loss)\n",
    "\n",
    "            optimizer = tf.train.AdamOptimizer(0.002, beta1=0.9, beta2=0.999, epsilon=1e-8).minimize(deep.loss)\n",
    "            '''optimizer=tf.train.RMSPropOptimizer(0.002)\n",
    "            grads_and_vars = optimizer.compute_gradients(deep.loss)'''\n",
    "            train_op = optimizer  # .apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "            sess.run(tf.initialize_all_variables())\n",
    "\n",
    "            if FLAGS.word2vec:\n",
    "                # initial matrix with random uniform\n",
    "                u = 0\n",
    "                initW = np.random.uniform(-1.0, 1.0, (len(vocabulary_user), FLAGS.embedding_dim))\n",
    "                # load any vectors from the word2vec\n",
    "                print(\"Load word2vec u file {}\\n\".format(FLAGS.word2vec))\n",
    "                with open(FLAGS.word2vec, \"rb\") as f:\n",
    "                    header = f.readline()\n",
    "                    vocab_size, layer1_size = map(int, header.split())\n",
    "                    binary_len = np.dtype('float32').itemsize * layer1_size\n",
    "                    for line in xrange(vocab_size):\n",
    "                        word = []\n",
    "                        while True:\n",
    "                            ch = f.read(1)\n",
    "                            if ch == ' ':\n",
    "                                word = ''.join(word)\n",
    "                                break\n",
    "                            if ch != '\\n':\n",
    "                                word.append(ch)\n",
    "                        idx = 0\n",
    "\n",
    "                        if word in vocabulary_user:\n",
    "                            u = u + 1\n",
    "                            idx = vocabulary_user[word]\n",
    "                            initW[idx] = np.fromstring(f.read(binary_len), dtype='float32')\n",
    "                        else:\n",
    "                            f.read(binary_len)\n",
    "                sess.run(deep.W1.assign(initW))\n",
    "                initW = np.random.uniform(-1.0, 1.0, (len(vocabulary_item), FLAGS.embedding_dim))\n",
    "                # load any vectors from the word2vec\n",
    "                print(\"Load word2vec i file {}\\n\".format(FLAGS.word2vec))\n",
    "\n",
    "                item = 0\n",
    "                with open(FLAGS.word2vec, \"rb\") as f:\n",
    "                    header = f.readline()\n",
    "                    vocab_size, layer1_size = map(int, header.split())\n",
    "                    binary_len = np.dtype('float32').itemsize * layer1_size\n",
    "                    for line in xrange(vocab_size):\n",
    "                        word = []\n",
    "                        while True:\n",
    "                            ch = f.read(1)\n",
    "                            if ch == ' ':\n",
    "                                word = ''.join(word)\n",
    "                                break\n",
    "                            if ch != '\\n':\n",
    "                                word.append(ch)\n",
    "                        idx = 0\n",
    "                        if word in vocabulary_item:\n",
    "                            item = item + 1\n",
    "                            idx = vocabulary_item[word]\n",
    "                            initW[idx] = np.fromstring(f.read(binary_len), dtype='float32')\n",
    "                        else:\n",
    "                            f.read(binary_len)\n",
    "\n",
    "                sess.run(deep.W2.assign(initW))\n",
    "\n",
    "            l = (train_length / FLAGS.batch_size) + 1\n",
    "            print( l)\n",
    "            ll = 0\n",
    "            epoch = 1\n",
    "            best_mae = 5\n",
    "            best_rmse = 5\n",
    "            train_mae = 0\n",
    "            train_rmse = 0\n",
    "\n",
    "            pkl_file = open(FLAGS.train_data, 'rb')\n",
    "\n",
    "            train_data = pickle.load(pkl_file)\n",
    "\n",
    "            train_data = np.array(train_data)\n",
    "            pkl_file.close()\n",
    "\n",
    "            pkl_file = open(FLAGS.valid_data, 'rb')\n",
    "\n",
    "            test_data = pickle.load(pkl_file)\n",
    "            test_data = np.array(test_data)\n",
    "            pkl_file.close()\n",
    "\n",
    "            data_size_train = len(train_data)\n",
    "            data_size_test = len(test_data)\n",
    "            batch_size = 100\n",
    "            ll = int(len(train_data) / batch_size)\n",
    "\n",
    "            for epoch in range(40):\n",
    "                # Shuffle the data at each epoch\n",
    "                shuffle_indices = np.random.permutation(np.arange(data_size_train))\n",
    "                shuffled_data = train_data[shuffle_indices]\n",
    "                for batch_num in range(ll):\n",
    "                    start_index = batch_num * batch_size\n",
    "                    end_index = min((batch_num + 1) * batch_size, data_size_train)\n",
    "                    data_train = shuffled_data[start_index:end_index]\n",
    "\n",
    "                    uid, iid, y_batch = zip(*data_train)\n",
    "\n",
    "                    u_batch = []\n",
    "                    i_batch = []\n",
    "                    for i in range(len(uid)):\n",
    "                        u_batch.append(u_text[uid[i][0]])\n",
    "                        i_batch.append(i_text[iid[i][0]])\n",
    "                    u_batch = np.array(u_batch)\n",
    "                    i_batch = np.array(i_batch)\n",
    "\n",
    "                    t_rmse, t_mae = train_step(u_batch, i_batch, uid, iid, y_batch, batch_num)\n",
    "                    current_step = tf.train.global_step(sess, global_step)\n",
    "                    train_rmse += t_rmse\n",
    "                    train_mae += t_mae\n",
    "\n",
    "                    if batch_num % 1000 == 0 and batch_num > 1:\n",
    "                        print(\"\\nEvaluation:\")\n",
    "                        print (batch_num)\n",
    "                        loss_s = 0\n",
    "                        accuracy_s = 0\n",
    "                        mae_s = 0\n",
    "\n",
    "                        ll_test = int(len(test_data) / batch_size) + 1\n",
    "                        for batch_num2 in range(ll_test):\n",
    "                            start_index = batch_num2 * batch_size\n",
    "                            end_index = min((batch_num2 + 1) * batch_size, data_size_test)\n",
    "                            data_test = test_data[start_index:end_index]\n",
    "\n",
    "                            userid_valid, itemid_valid, y_valid = zip(*data_test)\n",
    "\n",
    "                            u_valid = []\n",
    "                            i_valid = []\n",
    "                            for i in range(len(userid_valid)):\n",
    "                                u_valid.append(u_text[userid_valid[i][0]])\n",
    "                                i_valid.append(i_text[itemid_valid[i][0]])\n",
    "                            u_valid = np.array(u_valid)\n",
    "                            i_valid = np.array(i_valid)\n",
    "\n",
    "                            loss, accuracy, mae = dev_step(u_valid, i_valid, userid_valid, itemid_valid, y_valid)\n",
    "                            loss_s = loss_s + len(u_valid) * loss\n",
    "                            accuracy_s = accuracy_s + len(u_valid) * np.square(accuracy)\n",
    "                            mae_s = mae_s + len(u_valid) * mae\n",
    "                        print (\"loss_valid {:g}, rmse_valid {:g}, mae_valid {:g}\".format(loss_s / test_length,\n",
    "                                                                                         np.sqrt(\n",
    "                                                                                             accuracy_s / test_length),\n",
    "                                                                                         mae_s / test_length))\n",
    "\n",
    "                print (str(epoch) + ':\\n')\n",
    "                print(\"\\nEvaluation:\")\n",
    "                print (\"train:rmse,mae:\", train_rmse / ll, train_mae / ll)\n",
    "                train_rmse = 0\n",
    "                train_mae = 0\n",
    "\n",
    "                loss_s = 0\n",
    "                accuracy_s = 0\n",
    "                mae_s = 0\n",
    "\n",
    "                ll_test = int(len(test_data) / batch_size) + 1\n",
    "                for batch_num in range(ll_test):\n",
    "                    start_index = batch_num * batch_size\n",
    "                    end_index = min((batch_num + 1) * batch_size, data_size_test)\n",
    "                    data_test = test_data[start_index:end_index]\n",
    "\n",
    "                    userid_valid, itemid_valid, y_valid = zip(*data_test)\n",
    "                    u_valid = []\n",
    "                    i_valid = []\n",
    "                    for i in range(len(userid_valid)):\n",
    "                        u_valid.append(u_text[userid_valid[i][0]])\n",
    "                        i_valid.append(i_text[itemid_valid[i][0]])\n",
    "                    u_valid = np.array(u_valid)\n",
    "                    i_valid = np.array(i_valid)\n",
    "\n",
    "                    loss, accuracy, mae = dev_step(u_valid, i_valid, userid_valid, itemid_valid, y_valid)\n",
    "                    loss_s = loss_s + len(u_valid) * loss\n",
    "                    accuracy_s = accuracy_s + len(u_valid) * np.square(accuracy)\n",
    "                    mae_s = mae_s + len(u_valid) * mae\n",
    "                print (\"loss_valid {:g}, rmse_valid {:g}, mae_valid {:g}\".format(loss_s / test_length,\n",
    "                                                                                 np.sqrt(accuracy_s / test_length),\n",
    "                                                                                 mae_s / test_length))\n",
    "                rmse = np.sqrt(accuracy_s / test_length)\n",
    "                mae = mae_s / test_length\n",
    "                if best_rmse > rmse:\n",
    "                    best_rmse = rmse\n",
    "                if best_mae > mae:\n",
    "                    best_mae = mae\n",
    "                print(\"\")\n",
    "            print('best rmse:', best_rmse)\n",
    "            print('best mae:', best_mae)\n",
    "\n",
    "    print('end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
